"""
Adzuna API Client for Job Market Analysis

This module provides a comprehensive client for interacting with the Adzuna job search API.
It supports both single-threaded and multi-threaded job fetching with various scoping options.

Key Features:
- Single and multi-threaded job fetching
- Configurable search parameters
- Error handling and logging
- Data processing and formatting
- File export capabilities
"""

import os, requests, json, datetime
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor
import logging
import pandas as pd

import sys
from pathlib import Path

log = logging.getLogger(__name__)
load_dotenv()

PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"

from adzuna_api_configs import ADZUNA_API_PRESETS


def configure_logging(level="INFO", log_file="adzuna_api.log"):
    """
    Configures logging for the application.

    Args:
        level: Logging level (e.g., "INFO", "DEBUG", "WARNING")
        log_folder: Directory to save log files
        log_file: Name of the log file
    """

    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    log_path = LOGS_DIR / log_file

    logging.basicConfig(
        level=getattr(logging, level),
        format="%(asctime)s %(levelname)s %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(), logging.FileHandler(log_path, "a")],
    )


configure_logging()


class AdzunaAPI:
    """Adzuna API client for job search"""

    def __init__(
        self,
        app_id: str = None,
        app_key: str = None,
    ):

        self.app_id = app_id
        self.app_key = app_key
        self.base_url = "https://api.adzuna.com/v1/api"

        if not self.app_id or not self.app_key:
            raise ValueError("No ADZUNA creds set.")

    def search_jobs(
        self,
        country: str = "gb",
        category: Optional[str] = None,
        results_per_page: int = 50,
        sort_by: str = None,
        what_or: str = None,
        what_and: str = None,
        mode: str = "single_thread",
        scope: str = "single_page",
        page: int = 1,  # goes with single_page single_tread
        pages: list = None,  # goes with multiple_pages scope
        max_workers: int = None,  # goes with multiple_threads
        formated: bool = False,
        page_list: list = None,
    ) -> Dict[str, Any]:
        """
        Searches for jobs using the Adzuna API.

        Args:
            country: Country code for job search (default: "gb")
            category: Job category filter (optional)
            results_per_page: Number of results per page (max 50)
            sort_by: Sort results by a specific field (optional)
            what_or: Search for jobs matching any of these keywords (optional)
            what_and: Search for jobs matching all of these keywords (optional)
            mode: Execution mode ("single_thread", "multithreading")
            scope: Scope of search ("single_page", "all_pages", "page_list")
            page: Page number for single page search (ignored for multiple_pages)
            pages: List of page numbers for multiple page search (autogenerated for all_pages)
            max_workers: Number of workers for multithreading (ignored for single_thread)
            formated: If True, returns formatted job results; otherwise raw API results
            page_list: List of specific page numbers to fetch (only for "page_list" scope)

        Returns:
            Tuple containing:
                - List of job results (formatted or raw based on 'formated' param)
                - List of pages that encountered errors

        Raises:
            ValueError: If invalid parameters are provided
            requests.RequestException: If API request fails
            KeyboardInterrupt: If the user interrupts the process, stops the process and saves intermediate results

        Example:
            >>> api = AdzunaAPI(app_id, app_key)
            >>> results, errors = api.search_jobs(country="us", what_or="python")
        """

        # build parameters dictionaty for API request
        params = {
            "app_id": self.app_id,
            "app_key": self.app_key,
            "results_per_page": min(results_per_page, 50),
        }
        if what_or:
            params["what_or"] = what_or
        if what_and:
            params["what_and"] = what_and
        if sort_by:
            params["sort_by"] = sort_by
        if category:
            params["category"] = category

        # TODO: Add input validation for search parameters

        if mode == "multithreading":
            results, page_error_list = self._fetch_jobs_multithreading(
                params, scope, max_workers, page_list
            )
        elif mode == "single_thread":
            if scope == "single_page":
                results = [
                    self._fetch_single_page(
                        f"{self.base_url}/jobs/{country}/search/{page}", params, page
                    )
                ]

                page_error_list = []
            else:
                results, page_error_list = self._fetch_jobs_multithreading(
                    params, scope, 1, page_list
                )  # will rewrite to avoid threads if it will run for too long

        # format results if needed and return final
        return (
            self._process_job_results(results) if formated else results
        ), page_error_list

    def _fetch_jobs_multithreading(
        self,
        params: Dict[str, Any],
        scope: str,
        max_workers: int,
        page_list: list,
        country: str = "gb",
    ):

        results = []
        page_error_list = []
        # we want to create index array but in case of "all", we don't know the count
        if scope == "all_pages":
            # fetch first result to find out the count and build page_list
            tester = self._fetch_single_page(
                f"{self.base_url}/jobs/{country}/search/{1}", params, 1
            )
            if "error" in tester:
                raise ValueError("failed to fetch test page, please try again")

            page_num = tester["count"] // 50 + (1 if tester["count"] % 50 else 0)
            index_list = range(2, page_num + 1)
            results.append(tester)
        elif scope == "page_list":
            index_list = page_list
        elif scope == "single_page":
            raise ValueError("don't use single page with multithreading")

        # separate the index_list into batches of max_workers size
        batch_index_list = [
            index_list[i : i + max_workers]
            for i in range(0, len(index_list), max_workers)
        ]

        for batch in batch_index_list:
            try:
                with ThreadPoolExecutor(max_workers=len(batch)) as executor:

                    page_results = list(
                        executor.map(
                            self._fetch_single_page,
                            [
                                f"{self.base_url}/jobs/{country}/search/{p}"
                                for p in batch
                            ],
                            [params for i in batch],
                            batch,
                        )
                    )

                    # need to get the pages with errors
                    for i in page_results:
                        if "error_page" in i:
                            page_error_list.append(i["error_page"])

                    results += list(page_results)

                    log.info(f"processed pages {batch[0]}-{batch[-1]}")
            except KeyboardInterrupt:
                log.info(
                    f"you interrupted, stopped on batch {batch},\n what_or: {params.get('what_or', 'None')} what_and: {params.get('what_and', 'None')}"
                )
                break
            except Exception as e:  # implement error handling and proper saving
                log.info(
                    f"unknow error in multithreading process: {e}\n stopped on batch {batch}, what_or: {params.get('what_or', 'None')} what_and: {params.get('what_and', 'None')}"
                )

        return results, page_error_list

    def _fetch_single_page(
        self, endpoint: str, params: Dict[str, Any], page: int = None
    ):
        """
        Fetches a single page of job results from the Adzuna API.

        Args:
            endpoint: The API endpoint to call.
            params: Dictionary of parameters to send with the request.
            page: The page number for the request.

        Returns:
            A dictionary containing the API response or an error dictionary.
        """
        try:
            response = requests.get(endpoint, params=params, timeout=30)
            response.raise_for_status()

            return response.json()  # return our desired info

        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            print(f"page: {page}")
            return {"error": str(e), "error_page": page}
        except requests.exceptions.RequestException as e:
            print(f"Error making request to Adzuna API: {e}")
            return {
                "error": str(e)
            }  # return the error dict, to not flip up the func return
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            return {"error": "Invalid JSON response"}

    # clean data for json or printing
    def _process_job_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Processes raw API job results to a clean, standardized list of dictionaries.

        Args:
            results: A list of dictionaries, each representing a job batch from a one API fetch.

        Returns:
            A list of cleaned job dictionaries.
        """

        all_results = []

        for result in results:
            if "error" in result:
                continue
            jobs = result.get("results", [])
            processed_jobs = []

            for job in jobs:
                # cleaning method
                processed_job = {
                    "id": job.get("id"),
                    "title": job.get("title"),
                    "company": job.get("company", {}).get("display_name"),
                    "location": job.get("location", {}).get("display_name"),
                    "description": job.get("description"),
                    "salary_min": job.get("salary_min"),
                    "salary_max": job.get("salary_max"),
                    "salary_currency": job.get("salary_currency"),
                    "created": job.get("created"),
                    "redirect_url": job.get("redirect_url"),
                    "category": job.get("category", {}).get("label"),
                    "contract_type": job.get("contract_type"),
                    "contract_time": job.get("contract_time"),
                }
                processed_jobs.append(processed_job)
            all_results += processed_jobs

        return all_results

    def save_jobs_to_file(
        self,
        results: List[Dict[str, Any]],
        filename: str = None,
        output_type: str = "parquet",
    ) -> str:
        """
        Saves processed job results to a JSON or Parquet file.

        Args:
            results: List of job dictionaries to save.
            filename: Optional custom filename for the saved file.
            output_type: File format ("json" or "parquet").

        Returns:
            Path to the saved file.

        Raises:
            ValueError: If results is not a list or output_type is unsupported
            IOError: If file writing fails
        """

        # Input validation
        if not isinstance(results, list):
            raise ValueError("Results must be a list of dictionaries")

        if output_type not in ["json", "parquet"]:
            raise ValueError("output_type must be 'json' or 'parquet'")

        if output_type == "parquet" and not hasattr(pd, "DataFrame"):
            raise ValueError("pandas is required for parquet output")

        # if no filename is provided, create a default one
        if not filename:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            if output_type == "json":
                filename = f"{len(results)}_adzuna_jobs_{timestamp}.json"
            elif output_type == "parquet":
                filename = f"{len(results)}_adzuna_jobs_{timestamp}.parquet"

        raw_data_dir = DATA_DIR / "raw"
        raw_data_dir.mkdir(parents=True, exist_ok=True)
        filepath = raw_data_dir / filename

        # save the results to the file
        try:
            if output_type == "json":
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
            elif output_type == "parquet":
                df = pd.DataFrame(results)

                df.to_parquet(filepath)

            print(f"Results saved to {filepath}")
            return filepath

        except IOError as e:
            raise IOError(f"Failed to save file {filepath}: {e}")
        except Exception as e:
            raise Exception(f"Unexpected error saving file {filepath}: {e}")


def main():
    """
    Main execution block for the Adzuna API client.
    """

    api = AdzunaAPI(os.getenv("ADZUNA_ID"), os.getenv("ADZUNA_KEY"))
    config = ADZUNA_API_PRESETS["test_multithread_1411"]
    results, page_error_list = api.search_jobs(**config)
    log.info(f"Page error list: {page_error_list}")
    api.save_jobs_to_file(results, output_type="parquet")


if __name__ == "__main__":
    try:

        main()
    except Exception as e:
        log.error(f"Error: {e}")
