"""
Adzuna API Client for Job Market Analysis

This module provides a comprehensive client for interacting with the Adzuna job search API.
It supports both single-threaded and multi-threaded job fetching with various scoping options.

Key Features:
- Single and multi-threaded job fetching
- Configurable search parameters
- Error handling and logging
- Data processing and formatting
- File export capabilities
"""

import os, requests, json, datetime
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor

import pandas as pd

import sys
from pathlib import Path

from job_pipeline.logging_conf import get_logger, setup_logging

from job_pipeline.config import ADZUNA_API_PRESETS, LOGS_DIR, ADZUNA_ID, ADZUNA_KEY

from job_pipeline.steps.io_utils import adzuna_save_raw_bronze

log = get_logger(__name__)


class AdzunaAPI:
    """Adzuna API client for job search"""

    def __init__(
        self,
        app_id: str = None,
        app_key: str = None,
    ):

        self.app_id = app_id
        self.app_key = app_key
        self.base_url = "https://api.adzuna.com/v1/api"

        if not self.app_id or not self.app_key:
            raise ValueError("No ADZUNA creds set.")

    def search_jobs(
        self,
        country: str = "gb",
        category: Optional[str] = None,
        results_per_page: int = 50,
        sort_by: str = None,
        what_or: str = None,
        what_and: str = None,
        mode: str = "single_thread",
        scope: str = "single_page",
        page: int = 1,  # goes with single_page single_tread
        pages: list = None,  # goes with multiple_pages scope
        max_workers: int = None,  # goes with multiple_threads
        formated: bool = False,
        page_list: list = None,
    ) -> Dict[str, Any]:
        """
        Searches for jobs using the Adzuna API.

        Args:
            country: Country code for job search (default: "gb")
            category: Job category filter (optional)
            results_per_page: Number of results per page (max 50)
            sort_by: Sort results by a specific field (optional)
            what_or: Search for jobs matching any of these keywords (optional)
            what_and: Search for jobs matching all of these keywords (optional)
            mode: Execution mode ("single_thread", "multithreading")
            scope: Scope of search ("single_page", "all_pages", "page_list")
            page: Page number for single page search (ignored for multiple_pages)
            pages: List of page numbers for multiple page search (autogenerated for all_pages)
            max_workers: Number of workers for multithreading (ignored for single_thread)
            formated: If True, returns formatted job results; otherwise raw API results
            page_list: List of specific page numbers to fetch (only for "page_list" scope)

        Returns:
            Tuple containing:
                - List of job results (formatted or raw based on 'formated' param)
                - List of pages that encountered errors

        Raises:
            ValueError: If invalid parameters are provided
            requests.RequestException: If API request fails
            KeyboardInterrupt: If the user interrupts the process, stops the process and saves intermediate results

        Example:
            >>> api = AdzunaAPI(app_id, app_key)
            >>> results, errors = api.search_jobs(country="us", what_or="python")
        """

        # build parameters dictionaty for API request
        params = {
            "app_id": self.app_id,
            "app_key": self.app_key,
            "results_per_page": min(results_per_page, 50),
        }
        if what_or:
            params["what_or"] = what_or
        if what_and:
            params["what_and"] = what_and
        if sort_by:
            params["sort_by"] = sort_by
        if category:
            params["category"] = category

        # TODO: Add input validation for search parameters

        if mode == "multithreading":
            results, page_error_list = self._fetch_jobs_multithreading(
                params, scope, max_workers, page_list
            )
        elif mode == "single_thread":
            if scope == "single_page":
                results = [
                    self._fetch_single_page(
                        f"{self.base_url}/jobs/{country}/search/{page}", params, page
                    )
                ]

                page_error_list = []
            else:
                results, page_error_list = self._fetch_jobs_multithreading(
                    params, scope, 1, page_list
                )  # will rewrite to avoid threads if it will run for too long

        # format results if needed and return final
        return (
            self._process_job_results(results) if formated else results
        ), page_error_list

    def _fetch_jobs_multithreading(
        self,
        params: Dict[str, Any],
        scope: str,
        max_workers: int,
        page_list: list,
        country: str = "gb",
    ):

        results = []
        page_error_list = []
        # we want to create index array but in case of "all", we don't know the count
        if scope == "all_pages":
            # fetch first result to find out the count and build page_list
            tester = self._fetch_single_page(
                f"{self.base_url}/jobs/{country}/search/{1}", params, 1
            )
            if "error" in tester:
                raise ValueError("failed to fetch test page, please try again")

            page_num = tester["count"] // 50 + (1 if tester["count"] % 50 else 0)
            index_list = range(2, page_num + 1)
            results.append(tester)
        elif scope == "page_list":
            index_list = page_list
        elif scope == "single_page":
            raise ValueError("don't use single page with multithreading")

        # separate the index_list into batches of max_workers size
        batch_index_list = [
            index_list[i : i + max_workers]
            for i in range(0, len(index_list), max_workers)
        ]

        for batch in batch_index_list:
            try:
                with ThreadPoolExecutor(max_workers=len(batch)) as executor:

                    page_results = list(
                        executor.map(
                            self._fetch_single_page,
                            [
                                f"{self.base_url}/jobs/{country}/search/{p}"
                                for p in batch
                            ],
                            [params for i in batch],
                            batch,
                        )
                    )

                    # need to get the pages with errors
                    for i in page_results:
                        if "error_page" in i:
                            page_error_list.append(i["error_page"])

                    results += list(page_results)

                    log.info(f"processed pages {batch[0]}-{batch[-1]}")
            except KeyboardInterrupt:
                log.info(
                    f"you interrupted, stopped on batch {batch},\n what_or: {params.get('what_or', 'None')} what_and: {params.get('what_and', 'None')}"
                )
                break
            except Exception as e:  # implement error handling and proper saving
                log.info(
                    f"unknow error in multithreading process: {e}\n stopped on batch {batch}, what_or: {params.get('what_or', 'None')} what_and: {params.get('what_and', 'None')}"
                )

        return results, page_error_list

    def search_jobs_robust(
        self,
        country: str = "gb",
        category: Optional[str] = None,
        results_per_page: int = 50,
        sort_by: str = None,
        what_or: str = None,
        what_and: str = None,
        mode: str = "single_thread",
        scope: str = "single_page",
        page: int = 1,  # goes with single_page single_tread
        pages: list = None,  # goes with multiple_pages scope
        max_workers: int = None,  # goes with multiple_threads
        formated: bool = False,
        page_list: list = None,
        max_retries: int = 5,
    ) -> Dict[str, Any]:

        jobs, error_list = self.search_jobs(
            country=country,
            category=category,
            results_per_page=results_per_page,
            sort_by=sort_by,
            what_or=what_or,
            what_and=what_and,
            mode=mode,
            scope=scope,
            page=page,
            pages=pages,
            max_workers=max_workers,
            formated=formated,
            page_list=page_list,
        )
        try:
            prev = 0
            while len(error_list) > 0:

                if len(error_list) == prev:
                    max_retries -= 1
                    if not max_retries:
                        log.info(f"max retries reached, stopping search")
                        break

                page_list = error_list

                missed_jobs, error_list = self.search_jobs(
                    country=country,
                    category=category,
                    results_per_page=results_per_page,
                    sort_by=sort_by,
                    what_or=what_or,
                    what_and=what_and,
                    mode=mode,
                    scope="page_list",
                    page=page,
                    pages=pages,
                    max_workers=max_workers,
                    formated=formated,
                    page_list=page_list,
                )

                jobs += missed_jobs

                prev = len(error_list)

        except KeyboardInterrupt:
            return jobs, error_list

        return jobs, error_list

    def _fetch_single_page(
        self, endpoint: str, params: Dict[str, Any], page: int = None
    ):
        """
        Fetches a single page of job results from the Adzuna API.

        Args:
            endpoint: The API endpoint to call.
            params: Dictionary of parameters to send with the request.
            page: The page number for the request.

        Returns:
            A dictionary containing the API response or an error dictionary.
        """
        try:
            response = requests.get(endpoint, params=params, timeout=30)
            response.raise_for_status()

            return response.json()  # return our desired info

        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            print(f"page: {page}")
            return {"error": str(e), "error_page": page}
        except requests.exceptions.RequestException as e:
            print(f"Error making request to Adzuna API: {e}")
            return {
                "error": str(e)
            }  # return the error dict, to not flip up the func return
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            return {"error": "Invalid JSON response"}

    # clean data for json or printing
    def _process_job_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Processes raw API job results to a clean, standardized list of dictionaries.

        Args:
            results: A list of dictionaries, each representing a job batch from a one API fetch.

        Returns:
            A list of cleaned job dictionaries.
        """

        all_results = []

        for result in results:
            if "error" in result:
                continue
            jobs = result.get("results", [])
            processed_jobs = []

            for job in jobs:
                # cleaning method
                processed_job = {
                    "id": (
                        str(job.get("id")) if job.get("id") is not None else None
                    ),  # Convert to string
                    "title": job.get("title"),
                    "company": job.get("company", {}).get("display_name"),
                    "location": job.get("location", {}).get("display_name"),
                    "description": job.get("description"),
                    "salary_min": job.get("salary_min"),
                    "salary_max": job.get("salary_max"),
                    "salary_currency": job.get("salary_currency"),
                    "created": job.get("created"),
                    "redirect_url": job.get("redirect_url"),
                    "category": job.get("category", {}).get("label"),
                    "contract_type": job.get("contract_type"),
                    "contract_time": job.get("contract_time"),
                }
                processed_jobs.append(processed_job)
            all_results += processed_jobs

        return all_results

    def to_df(
        self,
        results: List[Dict[str, Any]],
    ):
        df = pd.DataFrame(results)

        # Convert problematic columns to strings
        if "id" in df.columns:
            df["id"] = df["id"].astype(str)

        return df


def main():
    """
    Main execution block for the Adzuna API client.
    """
    name = "test_page_list"
    api = AdzunaAPI(ADZUNA_ID, ADZUNA_KEY)
    config = ADZUNA_API_PRESETS[name]

    results, error_list = api.search_jobs_robust(**config)

    if error_list:
        log.info(f"Page error list: {error_list}")

    results = api.to_df(results)

    file_name = adzuna_save_raw_bronze(results, name)
    log.info("file_name:", file_name)


if __name__ == "__main__":
    try:

        main()
    except Exception as e:
        log.error(f"Error: {e}")
